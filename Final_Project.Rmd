---
title: "Final_Project"
author: "Tessa Danehy, Josh Eiland, Chirag Kulkarni"
date: "11/17/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(tidyverse)
#install.packages("naniar")
#install.packages("finalfit")
library(finalfit)
library(naniar)
library(caret)
library(randomForest)
data = read.csv("train.csv")
teamdata = read.csv("NFLTeamData.csv")
# short = read.csv("short.csv")
clean = read.csv("Cleaned_NFL_Data.csv")
```

Since we are getting the data from the NFL Big Data Bowl competition, the database is pretty robust. After creating a missing values map (below), we can see that most of the information is complete, with the exception of weather data. 

Temperature (and to a lesser extent humidity) information seems to be missing at various points throughout the data. This is curious as there is weather data out there and we could likely find a way to populate those fields knowing the day and location of the game. We may first see how significant temperature is in predicting rush yardage to determine if completing the temp data is worth it. 

Following this, we looked at a summary of the data to determine 1) whether the data was in the right format and 2) if there were any unreasonable values. Again, the data from the NFL is pretty great so all the data came in the correct format (categoricals for team name, player name, location, etc). There are a few features that we may engineer though: age of player (from birth day) and minutes into game (from game clock). 

For some next steps, we want to take a look at each of the column variables critically and determine which we want to test in a potential model. Ultimately, we will be experimenting with many different variables but we may rule some out from the offset. After that, we would like to connect to a database (NFL API) to get player stats and/or rankings. There is data out there such as Average Yards Per Play for each player and that could certainly improve our model. 
```{r Assessing the data}
missing_plot(data)
# many of the values had empty strings instead of NAs, so we assigned them as NAs and ran missingness plot again 
data[data==''] <- NA
missing_plot(data)

summary(data)
```

In cleaning the data, we observed each variable in depth, checking its range of values to understand its meaning within the context of yardage gain and ensuring that its format and type was conducive to future usage in models.
```{r Cleaning the Data}
# removing all players that are not the rusher, and then removing columns with variables we deem unimportant
df <- filter(data, data$NflId==data$NflIdRusher) %>% select(-c(JerseyNumber,Stadium,Location,WindSpeed,WindDirection))
names(df)

# We conducted a bit of further examination of missingness - found that most of the missing values were in the same columns. We eyeballed the missingness
sum(is.na(df)) # 6178 missing values
missing_plot(df)
# Big offenders were Temperature (2206), GameWeather (1895) and Stadium Type (1497)
# Smaller ones were Field Position (292), Humidity (280), Offense Formation (5), and Defenders in the Box (3)
# These 7 together accounted for all of the total 6178 missing values in the data set
# We immediately excluded the rows with missing values for offense formation and defenders in the box since it was just a few rows
nrow(df) # 23171
df <- df[!is.na(df$OffenseFormation) & !is.na(df$DefendersInTheBox),]
nrow(df) # 23165
# There are six fewer rows, meaning that 2 were missing both of these variables

# We now wanted to take a closer look at the other 5 variables to determine whether they were missing at random. To start, we looked into whether the mean yards gained differed between plays for which these variables were provided versus missing
mean(df[!is.na(df$Temperature),which(colnames(df)=="Yards")])-mean(df[is.na(df$Temperature),which(colnames(df)=="Yards")]) # difference of .13 yards
mean(df[!is.na(df$GameWeather),which(colnames(df)=="Yards")])-mean(df[is.na(df$GameWeather),which(colnames(df)=="Yards")]) # difference of .18 yards
mean(df[!is.na(df$StadiumType),which(colnames(df)=="Yards")])-mean(df[is.na(df$StadiumType),which(colnames(df)=="Yards")]) # difference of -.26 yards
mean(df[!is.na(df$FieldPosition),which(colnames(df)=="Yards")])-mean(df[is.na(df$FieldPosition),which(colnames(df)=="Yards")]) # difference of -.26 yards
mean(df[!is.na(df$Humidity),which(colnames(df)=="Yards")])-mean(df[is.na(df$Humidity),which(colnames(df)=="Yards")]) # difference of -.53 yards
sd(df$Yards)
# None of these differences seem particularly significant given that the standard deviation in rushing yards among the whole population is 6.44 yards

# used the summary() or levels() function on each variable to get a sense of strange values or variables that may be useful. Code omitted for simplicity 
# conclusions: offence personnel and defense personnel are very helpful, we should break them down
head(levels(df$OffensePersonnel))

# cleaning Stadium type (makes new column called StadiumTypeNew)
levels(df$StadiumType)
outdoor <- c("Cloudy", "Domed, open", "Domed, Open", "Open", "Oudoor", "Ourdoor", "Outddors","Heinz Field", "Indoor, Open Roof", "Outdoor", "Outdoor Retr Roof-Open", "Outdoors", "Outdor","Outside","Retr. Roof - Open", "Retr. Roof-Open")
indoor <- c("Bowl", "Closed Dome", "Dome", "Dome, closed", "Domed", "Domed, closed", "Indoor","Indoor, Roof Closed", "Indoors", "Retr. Roof - Closed", "Retr. Roof Closed", "Retr. Roof-Closed","Retr. Roof-Closed", "Retractable Roof")
df$StadiumTypeNew <- NA
df[df$StadiumType %in% outdoor == TRUE, which(colnames(df)=="StadiumTypeNew")] <- 'Outdoor'
df[df$StadiumType %in% indoor == TRUE, which(colnames(df)=="StadiumTypeNew")] <- 'Indoor'
levels(as.factor(df$StadiumTypeNew))
df$StadiumType = NULL

# cleaning Turf (makes new column called TurfNew)
levels(df$Turf)
artificialTurf = c("A-Turf Titan","Artifical","Artificial", "Field turf", "Field Turf", "FieldTurf", "FieldTurf 360", "FieldTurf360","Twenty-Four/Seven Turf", "UBU Speed Series-S5-M", "UBU Sports Speed S5-M")
naturalGrass = c("DD GrassMaster", "grass", "Grass", "Natural", "natural grass", "Natural grass", "Natural Grass", "Naturall Grass", "SISGrass")
df$TurfNew <- NA
df[df$Turf %in% artificialTurf == TRUE, which(colnames(df)=="TurfNew")] <- 'Turf'
df[df$Turf %in% naturalGrass == TRUE, which(colnames(df)=="TurfNew")] <- 'Grass'
levels(as.factor(df$TurfNew))
df$Turf <-NULL

# cleaning Player Height- changing it to inches (new feature called HeightNew)
for (cur in 1:nrow(df)){
  height <- str_split_fixed(df$PlayerHeight[cur], '-', 2)
  height1 <- as.numeric(height[1])
  height2 <- as.numeric(height[2])
  df$HeightNew[cur] <- height1*12+height2
} 
df$PlayerHeight <- NULL

# cleaning GameWeather- sort into adverse (rain and snow) and non-adverse conditions (called WeatherNew)
levels(df$GameWeather)
adverse<- c("30% Chance of Rain", "Cloudy with periods of rain, thunder possible. Winds shifting to WNW, 10-20 mph.", "Cloudy, 50% change of rain", "Cloudy, chance of rain",  "Cloudy, light snow accumulating 1-3\"", "Cloudy, Rain" , "Heavy lake effect snow", "Light Rain", "Rain", "Rain Chance 40%","Rain likely, temps in low 40s.", "Rain shower", "Rainy","Scattered Showers", "Showers", "Snow")
df$WeatherNew = NA
df[df$GameWeather %in% adverse == TRUE, which(colnames(df)=="WeatherNew")] <- 'Adverse'
df[df$GameWeather %in% adverse == FALSE, which(colnames(df)=="WeatherNew")] <- 'Not Adverse'
df$GameWeather <- NULL

# checked all of the types and coerced where needed
sapply(df, class)
df$WeatherNew <- as.factor(df$WeatherNew)
df$TurfNew <- as.factor(df$TurfNew)
df$StadiumTypeNew <- as.factor(df$StadiumTypeNew)
df$Season <- as.factor(df$Season)
# Time values switched to numeric form rather than many factors
df$GameClock <- as.numeric(df$GameClock)
df$TimeSnap <- as.numeric(df$TimeSnap)
df$TimeHandoff <- as.numeric(df$TimeHandoff)
# note: We figured out that there were repetitive variables in the data; use NFL ID or Rusher ID in the model, only use the Player name
```

Since we were filtering out all player data except for that of the rusher, we needed a new way to measure the prowess of the offense (supporting the rusher) and the defense (attacking the rusher). We used ratings from Madden NFL for this. Madden NFL is a video game simulator of the NFL, and is widely regarded as the gold standard for mirroring skill levels of all players and teams. Using the offensive and defensive ratings from the 2017 and 2018 seasons, we created a new spreadsheet and loaded that data in to our main dataframe. 
```{r Merging NFL Team Data and Play Data}
#The ratings for each NFL team were taken from Madden, an NFL Video Game simulator that tracks a wide array of metrics of NFL teams. They are known to be a gold standard for representing NFL talent. 

for (current in 1:nrow(df)) {
  offTeam <- df$PossessionTeam[current] #Find out who is on offense
  defTeam <- teamdata$Teams[(ifelse(df$PossessionTeam[current]==df$HomeTeamAbbr[current], df[current, "VisitorTeamAbbr"], df[current, "HomeTeamAbbr"]))] #Find out who is on defense
  
  offRating <- ifelse(df$Season[current]=="2017", teamdata[teamdata$Teams==offTeam, 3], teamdata[teamdata$Teams==offTeam, 5]) #Find out off rating from season
  defRating <- ifelse(df$Season[current]=="2017", teamdata[teamdata$Teams==defTeam, 2], teamdata[teamdata$Teams==defTeam, 4]) #Find out def rating from season
  
  df$offRating[current] <- offRating #Store the rating in a new column
  df$defRating[current] <- defRating #Store the rating in a new column
}
```

In this section we wanted to generate several new features from the existing ones rather than using outside sources as above. We generated the average rushing yards per play of all earlier attempts by the given rusher, the age of the rusher, the number of yards to the goal line (touchdown), the point difference (how much the rushing team was winning or losing by at the time of the play), and the number of different offensive and defensive personnel on the field.
```{r Feature Engineering}
##First, for each player, we calculate average rushing yards per play for all plays prior to the current play. 
df$Time <- strptime(df$TimeHandoff, format='%Y-%m-%dT%H:%M:%S') #Convert all time strings to a type that can be compared (POSIXlt)

for (current in 1:nrow(df)){ #Run through every play
    current_time <- df[current, 'Time'] #Find out what the current date/time is
    current_ID <- df[current, 'NflIdRusher'] #Find out who the rusher is
    before_play <- subset(df, Time>current_time) #Create a subset of all plays that occurred before the selected play
    average <- mean(before_play$Yards[before_play$NflIdRusher==current_ID]) #Calculate the average of all the prior plays where the same rusher was playing 
    df$average[current]<-average #Store the average
  
  } #This ran through about 5 million data points. Needless to say, this took a while (about 30 minutes). 

# Next we need to calculate the age of the rusher at time of handoff
df$birth<-strptime(df$PlayerBirthDate, format='%m/%d/%Y')
for (current in 1:nrow(df)) { #Run through every play
    current_time <- (df[current, 'Time'])
    birth_day <- (df[current, 'birth'])
    df$age[current] <- floor((difftime(current_time, birth_day, units='weeks')/52))
  }

# Another important variable is the distance to the goal. The distance variable that exists tells us how far to the first down, but the larger scope is also important (teams make different calls based on how close they are to scoring)

levels(df$FieldPosition) <- levels(df$PossessionTeam) # error because extra level in Field position
df$FieldPosition <- droplevels(df$FieldPosition) # eliminating the empty "" level that prevented checking equality of the two columns
levels(df$FieldPosition) <- levels(df$PossessionTeam)

df$YardsToGoal <- ifelse(df$PossessionTeam==df$FieldPosition,100-df$YardLine,df$YardLine) # If the team is on their own side of the field, we need to subtract their current yardline from 100, otherwise their yardline is equivalent to the yards to the end zone

sum(df$Yards > df$YardsToGoal,na.rm=TRUE) # a sanity check which returns 0 as expected, since someone cannot run more yards than exist between them and a touchdown

# Following this, we wanted to create a feature for the point difference. 
# We noticed at this point that the team abbreviations are different for the possession team variable and the home/away variables (very problematic) in 4 cases (Arizona, Baltimore, Cleveland, and Houston), so we reassigned them to be the same to enable comparison across them

levels(df$HomeTeamAbbr) == levels(df$PossessionTeam)
# Arizona is ARI vs. ARZ, Baltimore is BAL vs. BLT, Cleveland is CLE vs. CLV, and Houston is HOU vs. HST - very odd

# We were planning to manually replace these instances but it turned out to be much easier - R could automatically translate them over by setting the levels equal
levels(df$HomeTeamAbbr) <- levels(df$PossessionTeam)
levels(df$VisitorTeamAbbr) <- levels(df$PossessionTeam)

# for (current in 1:nrow(df)) { df$point_diff[current] <- ifelse(toString(df$PossessionTeam[current])==toString(df$HomeTeamAbbr[current]), df$HomeScoreBeforePlay[current]-df$VisitorScoreBeforePlay[current], df$VisitorScoreBeforePlay[current]-df$HomeScoreBeforePlay[current])}
# original method for determining point differential

df$Point_Diff <- ifelse(df$PossessionTeam==df$VisitorTeamAbbr,df$VisitorScoreBeforePlay-df$HomeScoreBeforePlay,df$HomeScoreBeforePlay-df$VisitorScoreBeforePlay) # much faster than the for loop method

# We wanted to break up offensive personnel into the number of other key player positions: RBs, TEs, WRs
for (current in 1:nrow(df)) {
  WR <- as.numeric(substr(df[current, 'OffensePersonnel'],gregexpr(pattern='W', df[current, 'OffensePersonnel'])[[1]]-2, gregexpr(pattern='W', df[current, 'OffensePersonnel'])[[1]]-2))
  
  TE <- as.numeric(substr(df[current, 'OffensePersonnel'],gregexpr(pattern='T', df[current, 'OffensePersonnel'])[[1]]-2, gregexpr(pattern='T', df[current, 'OffensePersonnel'])[[1]]-2))
  
  RB <- as.numeric(substr(df[current, 'OffensePersonnel'],gregexpr(pattern='R', df[current, 'OffensePersonnel'])[[1]]-2, gregexpr(pattern='R', df[current, 'OffensePersonnel'])[[1]]-2))
  
  df$wr[current] <- WR
  df$te[current] <- TE
  df$rb[current] <- RB
}

# We also wanted to break up defensive personnel into the number of other key player positions: DLs, LBs, DBss
for (current in 1:nrow(df)) {
    DL <- as.numeric(substr(df[current, 'DefensePersonnel'],gregexpr(pattern='DL', df[current, 'DefensePersonnel'])[[1]]-2, gregexpr(pattern='DL', df[current, 'DefensePersonnel'])[[1]]-2))
    
    LB <- as.numeric(substr(df[current, 'DefensePersonnel'],gregexpr(pattern='LB', df[current, 'DefensePersonnel'])[[1]]-2, gregexpr(pattern='LB', df[current, 'DefensePersonnel'])[[1]]-2))
    
    DB <- as.numeric(substr(df[current, 'DefensePersonnel'],gregexpr(pattern='DB', df[current, 'DefensePersonnel'])[[1]]-2, gregexpr(pattern='DB', df[current, 'DefensePersonnel'])[[1]]-2))
    
    df$dl[current] <- DL
    df$lb[current] <- LB
    df$db[current] <- DB
  }

# Recategorizing the yardage to one of four categories
df$Cat <- "-"
# Negative plays
df[df$Yards < 0, which(colnames(df)=="Cat")] <- "Negative"
# Positive plays of less than 4 yards
df[df$Yards >= 0 & df$Yards < 4, which(colnames(df)=="Cat")] <- "Small"
# Plays of at least 4 but less than 10 yards
# This is the rate necessary to never face a 4th down
df[df$Yards >= 4 & df$Yards < 10, which(colnames(df)=="Cat")] <- "Medium"
# Plays gaining 10 or more yards - enough for a first down
   # (assuming no prior negative yardage since the last first down)
df[df$Yards >= 10, which(colnames(df)=="Cat")] <- "Large"
df$Cat = as.factor(df$Cat)
table(df$Cat) # We see an interesting distribution of plays - roughly equal parts in the large and negative groups with 4 times as many in small and 3 times as many in medium. This 1:4:3:1 breakdown was pretty lucky and neat.

# Also creating a binary categorization to utilize VarImp
df$Good <- "-"
df[df$Yards < 4, which(colnames(df)=="Good")] <- 0
df[df$Yards >= 4, which(colnames(df)=="Good")] <- 1
table(df$Good) # The breakdown reflects more "bad" plays than "good" ones but not by a huge margin
df$Good <- as.factor(df$Good)
```

Here we looked into the correlation of Yards with each of the quantitative variables.
```{r Exploratory Data Analysis}
df <- clean # this line is used to make sure the dataframe is equivalent to the correct previously saved version
# correlation only works with quantitative 
only_quant = df %>% select_if(is.numeric) %>% select_if(is.integer)
quant_cor = cor(only_quant)
quant_cor
# We see the strongest correlation with acceleration (positive), defenders in the box (negative), and distance to the first down (positive)
```
We started our regression analysis with typical linear regression, just to see what the mean squared error was as a baseline. For all of the following regressions, we used variables that are most important as determined by the RandomForest variable importance graph and the correlation values. We built the model, then predicted the values of yards and compared them to the actual yardage to obtain the MSE of the linear regrssion. The result was very high, in the 50s. Next we tried to use logistic regression and break the rushing yards into good plays (more than 4 yards obtained) or bad plays (less than 4 yards). This model actually did quite worse, with an MSE in the high 60s.

We began to explore other regression models that would work well with our high number of predictors with low significancce of these predictors. Shrinkage methods are used in this exact case scenario, so we gave both Lasso and Ridge regression a go. Both of these methods introduce a lambda "shrinkage pentalty" that introduces bias in order to decrease the variance of the model which we determined through cross validation. The result is that in ridge regression, many of the variables are shrunken towards zero, and in Lasso many of the variables are set to be exactly zero (also performing variable selection). We has to manipulate the dataframe to be in the specific format that glmnet takes. These methods did improve the MSE significantly, with a Lasso of around 40 and a Ridge regression MSE of 38. The Ridge regression likely did better because although many of the relationships between explanatory variables and Yards are slight, they are valuable in predicting and should not be forced to zero. We also have some multicolinearity in our variables as we saw in the exploratory data analysis, and with that Ridge Regression outperforms Lasso. 

```{R regression}
library(glmnet)

clean = read.csv("Cleaned_NFL_Data.csv")
cut <- createDataPartition(clean$Yards, p=0.8, list = FALSE)
train <- clean[cut,]
test <- clean[-cut,]

# first try linear regression
result<-lm(Yards~ FieldPosition + Dir+ X+ Y+ Team + S + A + YardsToGoal + GameClock + Down + Distance + Point_Diff + OffenseFormation + DefendersInTheBox+ average + Orientation + YardLine + DisplayName+ age, data=train)
summary(result)
lr_predicted = predict(result, data= test)
# calculate the MSE of the model
mean((lr_predicted - test$Yards)^2)

# next try logistic regression on the binary response (Good or not good play)
result_binary<-glm(Good~ FieldPosition + Dir+ X+ Y+ Team + S + A + YardsToGoal + GameClock + Down + Distance + Point_Diff + OffenseFormation + DefendersInTheBox+ average + Orientation + YardLine + DisplayName+ age, data=train, family="binomial")
summary(result_binary)
logistic_predicted = predict(result_binary, data=test)
# calculate MSE
mean((logistic_predicted - test$Yards)^2)

####### SHRINKAGE METHODS
# wanted to use Ridge and LASSO because they are supposed to do well when many of the predictors are near zero and there are many dimensions

# the glmnet function requires that the response and predictors are separated in a specific way. In order to accomplish this, I had to make a new dataframe with only the variables I would use. 
vect = c("Yards", "Team", "S", "A","YardsToGoal","GameClock","Down", "Distance", "Point_Diff", "OffenseFormation", "DefendersInTheBox", "average", "Orientation", "YardLine", "age", "DisplayName","FieldPosition", "Dir", "X", "Y")
shrinkage_data = clean[,which(colnames(clean) %in% vect)]
shrinkage_data= na.omit(shrinkage_data)

x<-model.matrix(Yards~.,data=shrinkage_data)[,-16]
y<-shrinkage_data$Yards

# have to make a new random selection of train/test for this data, again because lasso and ridge needs the data in a specific format
set.seed(100)
cut<-sample.int(nrow(shrinkage_data), floor(.80*nrow(shrinkage_data)), replace = F) 
x.train <- x[cut,]
x.test <- x[-cut,]
y.train <- y[cut]
y.test <- y[-cut]

# LASSO
# run the lasso model 
set.seed(100)
library(glmnet)
lasso.model<-glmnet(x.train,y.train,alpha=1,lambda=10,thresh = 1e-14)
# fine tune the lambda parameter with cross validation 
cv.output <-cv.glmnet(x.train,y.train,alpha=1)
plot(cv.output)
bestlambda = cv.output$lambda.min
bestlambda
# Test MSE using the optimal lambda
lasso.pred<-predict(lasso.model,s=bestlambda,newx=x.test)
mean((lasso.pred-y.test)^2)


# Ridge Regression
# run the ridge model
set.seed(100)
ridge.model<-glmnet(x.train,y.train,alpha=0,thresh = 1e-14)
# fine tune the lambda parameter with cross validation
cv.out<-cv.glmnet(x.train,y.train,alpha=0)
plot(cv.out)
bestlambda<-cv.out$lambda.min
bestlambda
# Test MSE using the optimal lambda
ridge.preds<-predict(ridge.model,s=bestlambda,newx=x.test)
mean((ridge.preds-y.test)^2)

# viewing which variables are most important on best model 
coefs_ridge = coef(ridge.model)[,1]
coefs_ridge = sort(abs(coefs_ridge), decreasing = F)
coefs_ridge
```





Next we developed a whole slew of random forest models and used these models to take a look at the importance of different variables. We found some that the variables highlighted above were as expected among the most important, but also that some other less straightforward features were important while those we expected to be significant were often not. We ended up developing a predictive model that predicted the test set yardage with an average of only 1.52 yards in error.
```{r Random Forest and VarImp}
# Here we create a data partition in order to make training and test sets from the full data set
cut <- createDataPartition(df$Yards, p=0.8, list = FALSE)
train <- df[cut,]
test <- df[-cut,]

# We start with a basic random forest using only information about the player's location and motion
rf0 <- randomForest(Yards ~ Team + X + Y + S + A + Dis + Orientation + Dir, data=train,mtry=6,importance=TRUE) # only explains 3.39% of variation
varImpPlot(rf0,type=2) # Shows X, A, Orientation, and Dir followed by Y and S and more distantly by Dis then Team very little

# Next we included only variables we believed to be important with a few featured ones (point difference and yards to goal) 
rf1 <- randomForest(Yards ~ Team + S + A + YardsToGoal + GameClock + Down + Distance + Point_Diff + OffenseFormation + DefendersInTheBox, data=train,mtry=6,importance=TRUE) # explains even less, 2.55% of variation

# The decrease in model performance led us to want a deeper dive into variable importance, but this wasn't possible with the continuous outcome forest, so we used the binary outcome to get a sense
rfimp <- randomForest(Good ~ Team + S + A + YardsToGoal + GameClock + Down + Distance + Point_Diff + OffenseFormation + DefendersInTheBox, data=train,mtry=6,importance=TRUE)
varImpPlot(rfimp,type=2)
# The variable importance plot shows the most important variables as Acceleration, Speed, GameClock, and YardsToGoal, followed distantly by Points_Diff and then all the other variables

# A quick takeaway was that useful variables weren't always predictable, so we decided it was worthwhile to look across all of the originally provided variables that weren't repeats of others (teams) or too diversified for the 53 level filter (player name, college name). Some were too messy (stadium type) or not in the right form originally (height) - all altered or engineered variables were looked at later.
cols <- c("Team","X","Y","S","A","Dis","Orientation","Dir","YardLine", "Quarter","GameClock","Down","Distance","FieldPosition","HomeScoreBeforePlay","VisitorScoreBeforePlay","OffenseFormation","DefendersInTheBox","PlayDirection","TimeHandoff","Yards","PlayerWeight","Position","Week","Temperature","Humidity")
trimmedtrain <- train[,cols]
trimmedtrain <- trimmedtrain[complete.cases(trimmedtrain),]
rfall <- randomForest(Yards ~ ., data=trimmedtrain)
# This model predicted 5.24% of variation, better than previous marks.
varImpPlot(rfall,type=2)
# Most important is Field Position, then Acceleration, then X

slimtrain <- train[!is.na(train$YardsToGoal),]
# The following segment was a useful exercise in accomodating NAs - in this case for stadium type
summary(slimtrain$StadiumTypeNew)
levels(slimtrain$StadiumTypeNew)
# We created a third column that could serve as a factor without throwing NA errors but also remaining separate from known values
levels(slimtrain$StadiumTypeNew) <- c(levels(slimtrain$StadiumTypeNew),"Unknown") # manually adding the new factor level
levels(slimtrain$StadiumTypeNew)
slimtrain[is.na(slimtrain$StadiumTypeNew),which(colnames(slimtrain)=="StadiumTypeNew")] <- "Unknown"
summary(slimtrain$StadiumTypeNew)
slimtrain <- slimtrain[!is.na(slimtrain$average),]

# Next we built a model with all of our engineered variables 
rfeng <- randomForest(Yards ~ offRating + defRating + YardsToGoal + Point_Diff + wr + te +rb + StadiumTypeNew + TurfNew + HeightNew + WeatherNew + dl + lb + db + age + average, data=clean, mtry=6, importance=TRUE)
# Good, this now explains -2.7% of the variation
varImpPlot(rfeng,type=2)

# Combining the best features from the previous two models
best <- c("FieldPosition", "A", "X", "Dir", "TimeHandoff", "S", "Orientation", "Y", "GameClock", "average", "YardsToGoal", "Point_Diff", "defRating", "offRating", "age", "HeightNew","Yards")
btrain <- df[,best]
btrain <- btrain[complete.cases(btrain),]

# Here we combined the top features from each of the two previous models
rfcomb <- randomForest(Yards ~ FieldPosition + A + X + Dir + TimeHandoff + S + Orientation + Y + GameClock + average + YardsToGoal + Point_Diff + defRating + offRating + age + HeightNew, data=btrain,mtry=6,importance=TRUE)
# The accuracy dropped a bit to 4.6%
varImpPlot(rfcomb,type=2)

# Creating a binary random forest model to test accuracy at detecting big plays
binbest <- c(best,"Good")
binbest <- binbest[-which(binbest=="Yards")]
bintrain <- df[,binbest]
bintrain <- bintrain[complete.cases(bintrain),]
rfcombin <- randomForest(Good ~ FieldPosition + A + X + Dir + TimeHandoff + S + Orientation + Y + GameClock + average + YardsToGoal + Point_Diff + defRating + offRating + age + HeightNew, data=bintrain,mtry=6,importance=TRUE)
# The model classifies roughly 63% of plays correctly as 
varImpPlot(rfcombin,type=2)

# Creating a categorical random forest model to test accuracy at 
catbest <- c(best,"Cat")
catbest <- catbest[-which(catbest=="Yards")]
cattrain <- df[,catbest]
cattrain <- cattrain[complete.cases(cattrain),]
rfcomcat <- randomForest(Cat ~ FieldPosition + A + X + Dir + TimeHandoff + S + Orientation + Y + GameClock + average + YardsToGoal + Point_Diff + defRating + offRating + age + HeightNew, data=cattrain,mtry=6,importance=TRUE)
# Correctly categorizes around 48% of runs into the right group (out of 4)
# Lots of error among the large and negative (least common) groups
varImpPlot(rfcomcat,type=2)

predcomb <- predict(rfcomb,newdata=test) 
diffcomb <- mean(abs(predcomb - test$Yards),na.rm=TRUE)
diffcomb
# The predictions are only off by an average of 1.52 yards
```
